{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S1 J4 ? Performance (Cache + Partition)\n",
        "\n",
        "This notebook demonstrates basic performance techniques: partitioning, caching, and inspecting plans.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.appName(\"performance-cache-partition\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "data_path = \"../../data/example.csv\"\n",
        "\n",
        "raw = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .csv(data_path)\n",
        ")\n",
        "\n",
        "silver = (\n",
        "    raw\n",
        "    .withColumn(\"signup_date\", F.to_date(\"signup_date\"))\n",
        "    .withColumn(\"spend\", F.col(\"spend\").cast(\"double\"))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Partitioning example\n",
        "print(\"Initial partitions:\", silver.rdd.getNumPartitions())\n",
        "by_plan = silver.repartition(4, \"plan\")\n",
        "print(\"After repartition:\", by_plan.rdd.getNumPartitions())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache to avoid recomputation across actions\n",
        "by_plan.cache()\n",
        "\n",
        "# Materialize cache\n",
        "by_plan.count()\n",
        "\n",
        "# Confirm cache status\n",
        "print(\"Is cached:\", spark.catalog.isCached(by_plan.toString()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explain physical plan for a simple aggregation\n",
        "agg = (\n",
        "    by_plan\n",
        "    .groupBy(\"plan\")\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"users\"),\n",
        "        F.round(F.sum(\"spend\"), 2).alias(\"total_spend\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "agg.explain(\"formatted\")\n",
        "agg.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free cache when done\n",
        "by_plan.unpersist()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}