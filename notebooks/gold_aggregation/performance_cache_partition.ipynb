{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7a7d5f",
   "metadata": {},
   "source": [
    "# S1 J4 ? Performance (Cache + Partition)\n",
    "\n",
    "This notebook demonstrates basic performance techniques: partitioning, caching, and inspecting plans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20607bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/09 09:37:46 WARN Utils: Your hostname, MA-L-481079, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/02/09 09:37:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/09 09:37:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/09 09:37:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/02/09 09:37:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"performance-cache-partition\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5812640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data_path = \"../../data/example.csv\"\n",
    "\n",
    "raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(data_path)\n",
    ")\n",
    "\n",
    "silver = (\n",
    "    raw\n",
    "    .withColumn(\"signup_date\", F.to_date(\"signup_date\"))\n",
    "    .withColumn(\"spend\", F.col(\"spend\").cast(\"double\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee93041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial partitions: 1\n",
      "After repartition: 4\n"
     ]
    }
   ],
   "source": [
    "# Partitioning example\n",
    "print(\"Initial partitions:\", silver.rdd.getNumPartitions())\n",
    "by_plan = silver.repartition(4, \"plan\")\n",
    "print(\"After repartition:\", by_plan.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b16f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cached: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/09 09:43:45 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Cache to avoid recomputation across actions\n",
    "by_plan.cache()\n",
    "\n",
    "# Materialize cache\n",
    "by_plan.count()\n",
    "\n",
    "# Confirm cache status\n",
    "print(\"Is cached:\", by_plan.is_cached)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1dc4484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (10)\n",
      "+- HashAggregate (9)\n",
      "   +- HashAggregate (8)\n",
      "      +- InMemoryTableScan (1)\n",
      "            +- InMemoryRelation (2)\n",
      "                  +- AdaptiveSparkPlan (7)\n",
      "                     +- == Final Plan ==\n",
      "                        ResultQueryStage (6)\n",
      "                        +- ShuffleQueryStage (5), Statistics(sizeInBytes=904.0 B, rowCount=10)\n",
      "                           +- Exchange (4)\n",
      "                              +- Scan csv  (3)\n",
      "                     +- == Initial Plan ==\n",
      "                        Exchange (4)\n",
      "                        +- Scan csv  (3)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [2]: [plan#21, spend#26]\n",
      "Arguments: [plan#21, spend#26]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [name#17, age#18, city#19, signup_date#25, plan#21, is_active#22, spend#26], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [7]: [name#17, age#18, city#19, signup_date#20, plan#21, is_active#22, spend#23]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/lainislain/spark-databricks-learning/data/example.csv]\n",
      "ReadSchema: struct<name:string,age:int,city:string,signup_date:date,plan:string,is_active:boolean,spend:double>\n",
      "\n",
      "(4) Exchange\n",
      "Input [7]: [name#17, age#18, city#19, signup_date#20, plan#21, is_active#22, spend#23]\n",
      "Arguments: hashpartitioning(plan#21, 4), REPARTITION_BY_NUM, [plan_id=42]\n",
      "\n",
      "(5) ShuffleQueryStage\n",
      "Output [7]: [name#17, age#18, city#19, signup_date#20, plan#21, is_active#22, spend#23]\n",
      "Arguments: 0\n",
      "\n",
      "(6) ResultQueryStage\n",
      "Output [7]: [name#17, age#18, city#19, signup_date#20, plan#21, is_active#22, spend#23]\n",
      "Arguments: 1\n",
      "\n",
      "(7) AdaptiveSparkPlan\n",
      "Output [7]: [name#17, age#18, city#19, signup_date#20, plan#21, is_active#22, spend#23]\n",
      "Arguments: isFinalPlan=true\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [2]: [plan#21, spend#26]\n",
      "Keys [1]: [plan#21]\n",
      "Functions [2]: [partial_count(1), partial_sum(spend#26)]\n",
      "Aggregate Attributes [2]: [count#571L, sum#572]\n",
      "Results [3]: [plan#21, count#573L, sum#574]\n",
      "\n",
      "(9) HashAggregate\n",
      "Input [3]: [plan#21, count#573L, sum#574]\n",
      "Keys [1]: [plan#21]\n",
      "Functions [2]: [count(1), sum(spend#26)]\n",
      "Aggregate Attributes [2]: [count(1)#464L, sum(spend#26)#465]\n",
      "Results [3]: [plan#21, count(1)#464L AS users#455L, round(sum(spend#26)#465, 2) AS total_spend#456]\n",
      "\n",
      "(10) AdaptiveSparkPlan\n",
      "Output [3]: [plan#21, users#455L, total_spend#456]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "+----------+-----+-----------+\n",
      "|plan      |users|total_spend|\n",
      "+----------+-----+-----------+\n",
      "|Basic     |3    |148.99     |\n",
      "|Free      |2    |12.75      |\n",
      "|Pro       |3    |1099.75    |\n",
      "|Enterprise|2    |3540.1     |\n",
      "+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain physical plan for a simple aggregation\n",
    "agg = (\n",
    "    by_plan\n",
    "    .groupBy(\"plan\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"users\"),\n",
    "        F.round(F.sum(\"spend\"), 2).alias(\"total_spend\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "agg.explain(\"formatted\")\n",
    "agg.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "266f6906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, city: string, signup_date: date, plan: string, is_active: boolean, spend: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free cache when done\n",
    "by_plan.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44ea0f-6bfa-4bd4-af48-e1211579ac9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
